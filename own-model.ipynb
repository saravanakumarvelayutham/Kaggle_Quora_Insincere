{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Import necessary packages\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline, make_union, Pipeline\nfrom gensim.models import KeyedVectors\nimport operator \nimport pandas as pd\nfrom tqdm import tqdm\nimport re\nimport gensim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4141fd9d83f87a590b71a85c31540296945e5179"},"cell_type":"code","source":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmax_seq_len = 100\nEMBEDDING_MATRIX_FILE = 'embedding_matrix.npy'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9dca3c6cb104e5522357bc55f24dc326271b878b"},"cell_type":"code","source":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\n#Read the dataframes\ndef get_word_index(train):\n    train_df = train\n    ## fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_X))\n    return tokenizer.word_index\n\ndef load_and_prec(train,test):\n    train_df = train\n    test_df = test\n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    \n    ## fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n    \n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=max_seq_len)\n    test_X = pad_sequences(test_X, maxlen=max_seq_len)\n\n    ## Get the target values\n    train_y = train_df['target'].values\n    #shuffling the data\n    np.random.seed(2018)\n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    \n    return train_X, test_X, train_y\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36dc78dc5ffa5b319b3b5b4590b7e74578e81cf9"},"cell_type":"code","source":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\"apologised\" : \"apologize\"\n                ,\"demisexual\" : \"sexual\",\"Whate\" : \"What\",\"learnt\" : \"learn\", 'counselling': 'counseling', 'theatre': 'theater', \n                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\"sexualy\" : \"sexually\",\"Whydoes\" : \"why does\", \n                'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', \n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', \n                'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', \n                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ether', \n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n                'demonetisation': 'demonetization',\"honour\" : \"honor\" , \"honours\" : \"honor\",\"licence\" : \"license\" ,\n                \"cryptocurrency\" : \"money\",\"cryptocurrencies\" : \"money\",\"programme\": \"program\",\"B.E.\":\"education\",\n                      \"Redmi\" : \"Mobile\",\"mtech\":\"education\",\"Btech\" : \"education\",\"btech\" : \"education\",\"bitcoin\" : \"money\",\n                \"programr\" : \"programmer\",\"programrs\" : \"programmer\",\"realise\" : \"realize\" , \"behaviour\" : \"behavior\", \"blockchain\" : \"technology\"\n               ,\"Whatis\" : \"What is\",\"bcom\" : \"education\",\"favour\":\"favor\",\"cheque\":\"Check\"}\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82ad65c4096cce265b789d1093e15c4aff252e01"},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \n                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" , \n                       \"clg\" : \"college\",\"Demonetization\" : \"demonetization\",\"what's\" : \"what is\",\"don't\" : \"do not\",\n                       \"What's\" : \"What is\",\"Don't\" : \"Do not\",\"a\" : \"A\" , \"of\" : \"Of\",\"to\" : \"To\",\"and\" : \"And\",\"that's\":\"that is\",\"That's\" : \"That is\"}\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"478dc1995a262bb5d910dbbead7c98f85027f977"},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '?' : '',',':' ','\"':' ','(':' '}\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f'{p}')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa8dd07eefbf70596384538dcc939e8d98eeacb7"},"cell_type":"code","source":"def clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e280b3ce96c452106608350fa414f1cfb739254f"},"cell_type":"code","source":"def load_GoogleNews_index():\n    EMBEDDING_FILE = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n    embedding_index = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n    return embedding_index\n\ndef load_embedding_matrix(word_index,embeddings_index):\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index)) + 1\n    embedding_matrix = np.zeros((nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features or word not in embeddings_index:\n            continue\n        embedding_vector = embeddings_index.get_vector(word)            \n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14809b1ce221025ec240adf9cf7f1b2a04c3a84d"},"cell_type":"code","source":"embedding_index = load_GoogleNews_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dec948ff87d5f746e3b988eb3cedb5179bebaaaf"},"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b3dfd867c41d48bfd9260560b3658db207dd6f"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_contractions(x, contraction_mapping))\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntrain['question_text'] = train['question_text'].apply(lambda x : clean_text(x))\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: clean_numbers(x))\ntrain[\"question_text\"] = train[\"question_text\"].apply(lambda x: correct_spelling(x, mispell_dict))\n#train[\"question_text\"] = train[\"question_text\"].apply(lambda x: ' '.join([w for w in word_tokenize(x) if w not in ['a','to','of','and']]))\n\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_contractions(x, contraction_mapping))\n#test[\"question_text\"] = test[\"question_text\"].apply(lambda x: ' '.join([w for w in word_tokenize(x) if w not in ['a','to','of','and']]))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest['question_text'] = test['question_text'].apply(lambda x : clean_text(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\ntest[\"question_text\"] = test[\"question_text\"].apply(lambda x: correct_spelling(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"933126d1aa5ee83e48a403deb1308f51c9d8af0a"},"cell_type":"code","source":"#clean_contractions('and of a to',contraction_mapping)\n#test = pd.read_csv(\"../input/test.csv\")\n#test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_contractions(x, contraction_mapping))\n#test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n#test['question_text'] = test['question_text'].apply(lambda x : clean_text(x))\n#test[\"question_text\"] = test[\"question_text\"].apply(lambda x: clean_numbers(x))\n#test[\"question_text\"] = test[\"question_text\"].apply(lambda x: correct_spelling(x, mispell_dict))\n#test['question_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f4108424118baa1b932464f3220a533c2e1fa15"},"cell_type":"code","source":"sentences = train[\"question_text\"].apply(lambda x: x.split())\nvocab = build_vocab(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf08f60390ebd3fe54476ce7c9f074860576534e"},"cell_type":"code","source":"oov = check_coverage(vocab,embedding_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"023b6eb47a52a08a4ac0f8b66cfcf0108f16d376"},"cell_type":"code","source":"word_index = get_word_index(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75b14345891de7faa65f0f13379a8aec2e69f86d"},"cell_type":"code","source":"embedding_matrix = load_embedding_matrix(word_index,embedding_index)\nnp.shape(embedding_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60a2ecef3a6d8b1736d47b619f4ec6f6ce79bb3c"},"cell_type":"code","source":"train_X, test_X, train_y  = load_and_prec(train,test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a12d2ca12ae99f1cc553a40e2f570fead2dfc83b"},"cell_type":"code","source":"from keras import backend as K\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48dc0123493fc90d4203045a113cc9b172e33b46","scrolled":false},"cell_type":"code","source":"from keras.layers import Embedding,Input\nfrom keras.layers import MaxPool1D , Dense , Dropout , GlobalMaxPooling2D , Concatenate , CuDNNGRU , Dropout\nfrom keras.layers import Convolution1D , CuDNNLSTM , Bidirectional , Flatten , TimeDistributed , Reshape , Activation\nfrom keras.models import Model\nimport keras\nfrom keras.regularizers import L1L2 , l2\nfrom keras import optimizers\n\nfilter_sizes = [1,2,3,5]\nnum_filters = 36\nweight_decay = 1e-4\n\nembedding_layer = Embedding(max_features + 1,\n                            embed_size,\n                            weights = [embedding_matrix],\n                            trainable=False, name = 'Embedding')\n\nsequence_input = Input(shape=(max_seq_len,),name='Seq_Input1')\nembedded_sequences = embedding_layer(sequence_input)\n\n#embedded_sequences = Reshape((max_seq_len, embed_size, 1))(embedded_sequences)\n\nl_lstm = Bidirectional(CuDNNLSTM(150,return_sequences= True),name='LSTM')(embedded_sequences)\n\n#Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                    # kernel_initializer='he_normal', activation='elu')(x)\n\nmaxpool_pool = []\nfor i in range(len(filter_sizes)):\n    conv = Convolution1D(nb_filter=32,\n                         filter_length=filter_sizes[i],\n                         border_mode='valid',\n                         activation='relu',\n                         subsample_length=1)(l_lstm)\n    pool = MaxPool1D(pool_length=max_seq_len-filter_sizes[i]+1)(conv)\n    flattenMax = Flatten()(pool)\n    maxpool_pool.append(flattenMax)\n    \nz = Concatenate(axis=1)(maxpool_pool)   \nz = Dense(128)(z)\nz = Dropout(0.15)(z)\nz = Activation('relu')(z)\n\npreds = Dense(1, activation='sigmoid',name='Output')(z)\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel = Model(inputs = sequence_input,outputs = preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=adam,\n              metrics=[f1])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df8860e62840d91dc1f81c3caed29a665cb905bb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X_, val_X_ , train_Y_ , val_Y_ = train_test_split(train_X,train_y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a88253bab425920352c7a9182984d5b2cb894b"},"cell_type":"code","source":"#define callbacks\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88df246dbb68330c19e010ef554164c8b4312f8a","scrolled":true},"cell_type":"code","source":"#fit the model\nmodel.fit(train_X_, train_Y_, batch_size=512, epochs=5, verbose=1, shuffle=True ,validation_data= (val_X_,val_Y_), callbacks=callbacks_list)#, class_weight= class_weight )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60664c648a4f7d870ed42ab59d8c91711dedefd9"},"cell_type":"code","source":"predicted_Y = model.predict(test_X , batch_size= 500 , verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd185a1acb4a3b0e1955bdc2d94682216a7cf79"},"cell_type":"code","source":"predicted_Y = predicted_Y > 0.33","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7a13d30a457744e88e4e1bcea967958bccdf715"},"cell_type":"code","source":"pred_test_y = predicted_Y.astype(int)\nout_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9312871155ec33e77d442883c035140bae607176"},"cell_type":"code","source":"out_df[out_df.prediction==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b015f40ac04cfbdc0af4e7b21d8861d6bc3e9f96"},"cell_type":"code","source":"merged = pd.merge(test,out_df,on=[test.qid, out_df.qid])#out_df[out_df.qid == '93915191a313a164eceb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30ef52e566f3d65827105d04a32c689621a544d4"},"cell_type":"code","source":"merged[merged.qid_x == 'fe91a6259f58f7adc9d4']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec942defea75e6e6f38957c5f9a12b6506d94b39"},"cell_type":"code","source":"merged[merged.prediction == 1]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}